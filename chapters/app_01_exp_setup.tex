% !TeX root = ../main.tex
% Add the above to each chapter to make compiling the PDF easier in some editors.

\chapter{Experimental Setups}\label{appendix:experimental_setups}

\section*{Implementation Details and Computational Resources}

Our flows were primarily implemented in PyTorch \citep{paszkePyTorchImperativeStyle2019a}, building on the weight-space GNN and graph construction implementation of \citep{kofinasGraphNeuralNetworks2024} and with our custom flow matching implementation utilizing parts of the \texttt{torchcfm} package of \citep{tongImprovingGeneralizingFlowbased2023}. We have implemented a custom Euler solver to integrate our ODEs and used the \texttt{geoopt} package \citep{kochurovGeooptRiemannianOptimization2020} for geometric operations.

Throughout the experiments, we train all our models using a combination of NVIDIA A100 an H100 GPUs, with each model being trained on a single GPU. The training times range from around 6 hours for the smaller Euclidean flows to approximately 40 hours for the larger Geometric flows, although our implementation was not fully optimized for efficiency. 

\section*{Flow Between Gaussians (Section \ref{sec:gaussian_flow})}

\subsection*{Data}

\begin{itemize}
    \item $p_0 = \N(0, \mathbf{I}), p_1 = \N(1, \mathbf{I})$. 
    \item The model is an MLP with dimensions 30 - 16 - 16 - 2 and ReLU activations. 
\end{itemize}

\subsection*{Flow Architecture}
\begin{itemize}
    \item Relational Transformer \citep{diaoRelationalAttentionGeneralizing2023,kofinasGraphNeuralNetworks2024} with 5 layers and $d_E = 32$, GeLU activations \citep{hendrycksGaussianErrorLinear2023a}, one attention head. 
\end{itemize}

\subsection*{Flow/Training}
\begin{itemize}
    \item Independent coupling, $t \sim \text{Beta}(1, 2)$, Gaussian probability path with $\sigma = 0.001$, $x_1$ prediction.
    \item Trained for 20,000 iterations with batch size 16, using the Adam optimizer with initia learning rate 0.001.
\end{itemize}

\section*{UCI Classification (Section \ref{sec:uci_classification})}

\subsection*{Data}

\begin{itemize}
    \item $p_0 = \N(0, 0.1 \mathbf{I})$. To obtain samples from $p_1$, we sample 100 models from $p_0$ and train each independently for 50 epochs with Adam. We ignore the first 10 epochs, and record one sample every four iterations after that. 
    \item We use the same MLP architecture as the Gaussian experiments. 
\end{itemize}

\subsection*{Flow Architecture}
\begin{itemize}
    \item Relational Transformer \citep{diaoRelationalAttentionGeneralizing2023,kofinasGraphNeuralNetworks2024} with 5 layers and $d_E = 64$, GeLU activations \citep{hendrycksGaussianErrorLinear2023a}, one attention head. 
\end{itemize}

\subsection*{Flow/Training}
\begin{itemize}
    \item $t \sim \text{Beta}(1, 2)$, Gaussian probability path with $\sigma = 0.0001$, $x_1$ prediction.
    \item Trained for 250,000 iterations with batch size 32, using the Adam optimizer with initia learning rate 0.001.
\end{itemize}


\section*{MNIST Classification (Section \ref{sec:mnist_classification})}

\subsection*{Data}

\begin{itemize}
    \item $p_0 = \N(0, 0.1 \mathbf{I})$. For the target samples, we independently train 50 models for 25 epochs using SGD with momentum 0.9, learning rate 0.1, and weight decay 0.00001. We collect one sample every 10 iterations, discarding the first 5 epochs. 
    \item The base model is an MLP with 784 input and 10 output dimensions, and one hidden layer of 10 units. 
\end{itemize}

\subsection*{Flow Architecture}
\begin{itemize}
    \item Relational Transformer \citep{diaoRelationalAttentionGeneralizing2023,kofinasGraphNeuralNetworks2024} with 5 layers and $d_E = 32$, GeLU activations \citep{hendrycksGaussianErrorLinear2023a}, one attention head. 
\end{itemize}

\subsection*{Flow/Training}
\begin{itemize}
    \item $t \sim \text{Beta}(1, 2)$, Gaussian probability path with $\sigma = 0.00001$, $x_1$ prediction.
    \item Trained for $100,000$ iterations with batch size 8. 
\end{itemize}


